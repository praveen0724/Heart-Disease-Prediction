{"cells":[{"cell_type":"markdown","metadata":{"id":"D_drqRn3ebnZ"},"source":["<table align=\"left\" width=100%>\n","    <tr>\n","        <td width=\"10%\">\n","            <img src=\"title.png\">\n","        </td>\n","        <td>\n","            <div align=\"left\">\n","                <font color=\"#21618C\" size=8px>\n","                  <b>Heart Disease Prediction\n","                    </b>\n","                </font>\n","            </div>\n","        </td>\n","    </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"3OXjYS1webna"},"source":["## Problem Statement\n","\n","Heart disease is easier to treat when it is detected in the early stages. Machine learning techniques may aid a more efficient analysis in the prediction of the disease. Moreover, this prediction is one of the most central problems in medical, as it is one of the leading disease related to unhealthy lifestyle. So, an early prediction of this disease will be useful for a cure or averion. In this study, we experiment with the heart disease dataset to explore the machine learning algorithms and build an optimum model to predict the disease.                    "]},{"cell_type":"markdown","metadata":{"id":"AQO22FFqebnb"},"source":["## Data Definition\n","\n","Each attribute is a medical risk factor.\n","\n","    \n","**age**: Age of the patient - (Numerical)\n","\n","**gender**: Gender of the patient - (0,1) - (Male, Female) - (Categorical) \n"," \n","**chest_pain**: It refers to the chest pain experienced by the patient -(0,1,2,3) - (Categorical)\n","    \n","**rest_bps**: Blood pressure of the patient while resting(in mm/Hg) - (Numerical)\n","    \n","**cholestrol**: Patient's cholestrol level (in mg/dl) - (Numerical)\n","    \n","**fasting_blood_sugar**: Blood sugar of the patient while fasting - (>120mg/: = 1, otherwise = 0) - (Categorical)\n","    \n","**rest_ecg**: Potassium level (0,1,2) - (Categorical)\n","    \n","**thalach**: The patients maximum heart rate - (Numerical)\n","    \n","**exer_angina**: It refers to the exercise induced angina - (1=Yes, 0=No) - (Categorical)\n","    \n","**old_peak**: It is the ST depression induced by exercise relative to rest(ST relates to the position on ECG plots)  (Numerical)\n","    \n","**slope**:  It refers to the slope of the peak of the exercise ST Segment- (0,1,2) - (Categorical)\n","    \n","**ca**: Number of major vessels - (0,1,2,3,4) - (Categorical)\n","    \n","**thalassemia**: It refers to thalassemia which is a blood disorder - (0,1,2,3) - (Categorical)\n"," \n","**target**: Patient has heart disease or not - (1=Yes, 0=No) - (Target variable)"]},{"cell_type":"markdown","metadata":{"id":"trkPCukEebnc"},"source":["## Icon Legends\n","<table>\n","  <tr>\n","    <th width=\"25%\"> <img src=\"infer.png\" style=\"width:25%;\"></th>\n","    <th width=\"25%\"> <img src=\"alsoreadicon.png\" style=\"width:25%;\"></th>\n","    <th width=\"25%\"> <img src=\"todo.png\" style=\"width:25%;\"></th>\n","    <th width=\"25%\"> <img src=\"quicktip.png\" style=\"width:25%;\"></th>\n","  </tr>\n","  <tr>\n","    <td><div align=\"center\" style=\"font-size:120%\">\n","        <font color=\"#21618C\"><b>Inferences from outcome</b></font></div>\n","    </td>\n","    <td><div align=\"center\" style=\"font-size:120%\">\n","        <font color=\"#21618C\"><b>Additional Reads</b></font></div>\n","    </td>\n","    <td><div align=\"center\" style=\"font-size:120%\">\n","        <font color=\"#21618C\"><b>Lets do it</b></font></div>\n","    </td>\n","    <td><div align=\"center\" style=\"font-size:120%\">\n","        <font color=\"#21618C\"><b>Quick Tips</b></font></div>\n","    </td>\n","\n","</tr>\n","\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"2jRUOnVDwwwl"},"source":["## Table of Content\n","\n","1. **[Import Libraries](#import_lib)**\n","2. **[Set Options](#set_options)**\n","3. **[Read Data](#RD)**\n","4. **[Data Analysis and Preparation](#data_preparation)**\n","    - 4.1 - **[Understand the Dataset](#Data_Understanding)**\n","        - 4.1.1 - **[Data Type](#Data_Types)**\n","        - 4.1.2 - **[Summary Statistics](#Summary_Statistics)**\n","        - 4.1.3 - **[Distribution of Variables](#distribution_variables)**\n","        - 4.1.4 - **[Correlation](#correlation)**\n","        - 4.1.5 - **[Discover Outliers](#outliers)**\n","        - 4.1.6 - **[Missing Values](#Missing_Values)**      \n","    - 4.2 - **[Prepare the Data](#Data_Preparation)**\n","5. **[Decision Tree](#DecisionTree)**\n","    - 5.1 - **[Decision Tree](#DecisionTreeWFS)**\n","    - 5.2 - **[Prune a Decision Tree](#DecisionTreePruning)**\n","    - 5.3 - **[Decision Tree (using GridSearchCV)](#DecisionTreewithGridSearchCv)**\n","6. **[Conclusion and Interpretation](#conclusion)**"]},{"cell_type":"markdown","metadata":{"id":"AyalDMN-ebnc"},"source":["<a id='import_lib'></a>\n","# 1. Import Libraries"]},{"cell_type":"markdown","metadata":{"id":"O62j8dCiwwwm"},"source":["<table align=\"left\">\n","    <tr>\n","        <td width=\"8%\">\n","            <img src=\"todo.png\">\n","        </td>\n","        <td>\n","            <div align=\"left\", style=\"font-size:120%\">\n","                <font color=\"#21618C\">\n","                    <b>Import the required libraries and functions</b>\n","                </font>\n","            </div>\n","        </td>\n","    </tr>\n","</table>"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"lBgh1J0Mebnd"},"outputs":[],"source":["# suppress display of warnings\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","import os\n","\n","# 'Pandas' is used for data manipulation and analysis\n","import pandas as pd \n","\n","# 'Numpy' is used for mathematical operations on large, multi-dimensional arrays and matrices\n","import numpy as np\n","\n","# 'Matplotlib' is a data visualization library for 2D and 3D plots, built on numpy\n","import matplotlib.pyplot as plt\n","from matplotlib.colors import ListedColormap\n","\n","# 'Seaborn' is based on matplotlib; used for plotting statistical graphics\n","import seaborn as sns\n","\n","# import 'is_string_dtype' to check if the type of input is string  \n","from pandas.api.types import is_string_dtype\n","\n","# import various functions to perform classification\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn import metrics\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import cohen_kappa_score\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import roc_curve\n","from sklearn.linear_model import SGDClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.model_selection import GridSearchCV\n","from sklearn import tree\n","from sklearn.tree import export_graphviz\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.ensemble import AdaBoostClassifier\n","from sklearn.ensemble import GradientBoostingClassifier\n","\n","# import functions to perform logistic regression\n","import statsmodels\n","import statsmodels.api as sm\n","\n","# import functions to plot the decision tree\n","import pydotplus\n","from IPython.display import Image  \n","import graphviz"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JS_fvg4Uwwwq"},"outputs":[],"source":["! pip install pydotplus"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oIn4c1GJwwws"},"outputs":[],"source":["! pip install graphviz"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"8jhFmilpwwwt"},"outputs":[],"source":["# set the plot size using 'rcParams'\n","# once the plot size is set using 'rcParams', it sets the size of all the forthcoming plots in the file\n","# pass width and height in inches to 'figure.figsize' \n","plt.rcParams['figure.figsize'] = [15,8]"]},{"cell_type":"markdown","metadata":{"id":"Ll0ntaGOebng"},"source":["<a id='set_options'></a>\n","# 2. Set Options"]},{"cell_type":"markdown","metadata":{"id":"NdRrvYcEwwwt"},"source":["<table align=\"left\">\n","    <tr>\n","        <td width=\"8%\">\n","            <img src=\"todo.png\">\n","        </td>\n","        <td>\n","            <div align=\"left\", style=\"font-size:120%\">\n","                <font color=\"#21618C\">\n","                    <b>Now we make necessary changes to :<br><br>\n","1. Display complete data frames<br>\n","2. To avoid the exponential number<br>\n","                    </b>\n","                </font>\n","            </div>\n","        </td>\n","    </tr>\n","</table>"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"qJcw0J2Cebnh"},"outputs":[],"source":["# display all columns of the dataframe\n","pd.options.display.max_columns = None\n","\n","# display all rows of the dataframe\n","pd.options.display.max_rows = None\n","\n","# use below code to convert the 'exponential' values to float\n","np.set_printoptions(suppress=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"9nOxSTeawwwu"},"outputs":[],"source":["#os.chdir('/Users/suchita/DEsktop/PythonSession/HeartDiseaseCaseStudy')"]},{"cell_type":"markdown","metadata":{"id":"yPb9_if1ebnl"},"source":["<a id='RD'></a>\n","# 3. Read Data"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"ua8KGqLSw_y7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OADwdHKZwwwv"},"source":["<table align=\"left\">\n","    <tr>\n","        <td width=\"8%\">\n","            <img src=\"todo.png\">\n","        </td>\n","        <td>\n","            <div align=\"left\", style=\"font-size:120%\">\n","                <font color=\"#21618C\">\n","                    <b>Read and display data to get an insight into the data.</b>\n","                </font>\n","            </div>\n","        </td>\n","    </tr>\n","</table>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MDTwhMqAebnm"},"outputs":[],"source":["# read the excel data file \n","df_Heart = pd.read_csv(\"/content/drive/MyDrive/0.MKCE/4.Decision Tree/2 Project/Heart Disease Prediction/Dataset/HeartDisease.csv\")\n","\n","# display the top 5 rows of the dataframe\n","df_Heart.head()\n","\n","# Note: To display more rows, example 10, use head(10)"]},{"cell_type":"markdown","metadata":{"id":"vf8x7xFiwwww"},"source":["#### Dimensions of the data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k6QIB-Nowwww"},"outputs":[],"source":["# 'shape' function gives the total number of rows and columns in the data\n","df_Heart.shape"]},{"cell_type":"markdown","metadata":{"id":"SSHIYKY2ebns"},"source":["<a id='data_preparation'></a>\n","# 4. Data Analysis and Preparation"]},{"cell_type":"markdown","metadata":{"id":"l48tGdoxebnt"},"source":["<table align=\"left\">\n","    <tr>\n","        <td width=\"8%\">\n","            <img src=\"todo.png\">\n","        </td>\n","        <td>\n","            <div align=\"left\", style=\"font-size:120%\">\n","                <font color=\"#21618C\">\n","                    <b>Data preparation is the process of cleaning and transforming raw data before building predictive models. <br><br>\n","                        Here, we analyze and prepare data to perform classification techniques:<br>\n","                        1. Check data types. Ensure your data types are correct. Refer data definitions to validate <br>\n","                        2. If data types are not as per business definition, change the data types as per requirement <br>\n","                        3. Study summary statistics<br>\n","                        4. Distribution of variables<br>\n","                        5. Study correlation<br>\n","                        6. Detect outliers<br>\n","                        7. Check for missing values<br><br>\n","                        Note: It is an art to explore data, and one needs more and more practice to gain expertise in this area\n","                    </b>\n","                </font>\n","            </div>\n","        </td>\n","    </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"Y-baDwGWebnu"},"source":["<a id='Data_Understanding'></a>\n","## 4.1 Understand the Dataset"]},{"cell_type":"markdown","metadata":{"id":"O71NsesEebn3"},"source":["<a id='Data_Types'></a>\n","### 4.1.1 Data Type\n","The main data types in Pandas dataframes are the object, float, int64, bool, and datetime64. To understand each attribute of our data, it is always good for us to know the data type of each column."]},{"cell_type":"markdown","metadata":{"id":"rWIGih23wwwy"},"source":["<table align=\"left\">\n","    <tr>\n","        <td width=\"8%\">\n","            <img src=\"todo.png\">\n","        </td>\n","        <td>\n","            <div align=\"left\", style=\"font-size:120%\">\n","                <font color=\"#21618C\">\n","                    <b>In our dataset, we have numerical and categorical variables. The numeric variables should have data type 'int'/'float' while categorical variables should have data type 'object'.<br><br> \n","                        1. Check for the data type <br>\n","                        2. For any incorrect data type, change the data type with the appropriate type<br>\n","                        3. Recheck for the data type\n","                    </b>\n","                </font>\n","            </div>\n","        </td>\n","    </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"68w94ahlwwwy"},"source":["**1. Check for the data type**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TnYrzq7Yebn4","scrolled":false},"outputs":[],"source":["# 'dtypes' gives the data type for each column\n","df_Heart.dtypes"]},{"cell_type":"markdown","metadata":{"id":"uvOvxensebn6"},"source":["<table align=\"left\">\n","    <tr>\n","        <td width=\"8%\">\n","            <img src=\"infer.png\">\n","        </td>\n","        <td>\n","            <div align=\"left\", style=\"font-size:120%\">\n","                <font color=\"#21618C\">\n","                    <b>From the above output, it is clear to see that the data type of variables 'old_peak' is 'float64', and rest of the variables are identified as 'int64'.<br><br>\n","                        But as per data attribute information we have, 'gender', 'chest_pain', 'fasting_blood_sugar', 'rest_ecg', 'exer_angina', 'slope', 'ca', 'thalassemia' variables are categorical, which are wrongly interpreted as int64, so we convert these variables' data type to 'object'.</b>\n","                </font>\n","            </div>\n","        </td>\n","    </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"2LTAsWvPwwwz"},"source":["**2. Change the incorrect data type.**"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"ogPgThuFebn8","scrolled":true},"outputs":[],"source":["# use 'for' loop to change the data type of variables \n","for col in ['gender','chest_pain','fasting_blood_sugar','rest_ecg','exer_angina','slope','ca','thalassemia']:\n","     \n","    # use .astype() to change the data type\n","    df_Heart[col] = df_Heart[col].astype('object')"]},{"cell_type":"markdown","metadata":{"id":"7rG9ZJ7bebn-"},"source":["**3. Recheck the data type after the conversion.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zp6_mQOBebn_"},"outputs":[],"source":["# recheck the data types of all variables\n","df_Heart.dtypes"]},{"cell_type":"markdown","metadata":{"id":"udGKF9feeboB"},"source":["<table align=\"left\">\n","    <tr>\n","        <td width=\"8%\">\n","           <img src=\"infer.png\">\n","        </td>\n","        <td>\n","            <div align=\"left\", style=\"font-size:120%\">\n","                <font color=\"#21618C\">\n","                    <b>Now we have all the variables with corrected data type as required.</b>\n","                </font>\n","            </div>\n","        </td>\n","    </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"smKpu9Kzwww0"},"source":["<table align=\"left\">\n","    <tr>\n","        <td width=\"8%\">\n","            <img src=\"todo.png\">\n","        </td>\n","        <td>\n","            <div align=\"left\", style=\"font-size:120%\">\n","                <font color=\"#21618C\">\n","                    <b>For convenience, we shall split the target variable from the data frame.\n","Let dataframe 'df_target' be the dataframe containing the dependent variable and dataframe 'df_features' be the set of all independent variables.</b>\n","                </font>\n","            </div>\n","        </td>\n","    </tr>\n","</table>"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"ILGZCscuwww1"},"outputs":[],"source":["# splitting features and the target variable\n","# consider all the columns except 'target' using 'iloc'\n","df_features = df_Heart.iloc[:, df_Heart.columns != 'target']\n","\n","# consider the target variable\n","df_target = df_Heart.iloc[:,df_Heart.columns == 'target']"]},{"cell_type":"markdown","metadata":{"id":"uCHHAKbIwww1"},"source":["Use the dataframe containing features (df_features) for further analysis."]},{"cell_type":"markdown","metadata":{"id":"BFOkFedXeboC"},"source":["<a id='Summary_Statistics'></a>\n","### 4.1.2 Summary Statistics"]},{"cell_type":"markdown","metadata":{"id":"DrQ-poScwww1"},"source":["<table align=\"left\">\n","    <tr>\n","        <td width=\"8%\">\n","            <img src=\"todo.png\">\n","        </td>\n","        <td>\n","            <div align=\"left\", style=\"font-size:120%\">\n","                <font color=\"#21618C\">\n","                    <b> In our dataset, we have both numerical and categorical variables. Now we check for summary statistics of all the variables.<br><br>\n","                        1. For numerical variables, use the describe()<br>\n","                        2. For categorical variables, use the describe(include=object) \n","                        </b>\n","                </font>\n","            </div>\n","        </td>\n","    </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"zSPelw83www2"},"source":["**1. For numerical variables, use the describe()**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BCq-Il09eboC","scrolled":true},"outputs":[],"source":["# the describe() returns the statistical summary of the variables\n","# by default, it returns the summary of numerical variables\n","#We transpose the results for better readability\n","df_features.describe().transpose()"]},{"cell_type":"markdown","metadata":{"id":"BvaxGBkKeboF"},"source":["<table align=\"left\">\n","    <tr>\n","        <td width=\"8%\">\n","            <img src=\"infer.png\">\n","        </td>\n","        <td>\n","            <div align=\"left\", style=\"font-size:120%\">\n","                <font color=\"#21618C\">\n","<b>The above output illustrates the summary statistics of all the numeric variables namely mean, median (50%), standard deviation, minimum, and maximum values, along with the first and third quantiles.<br>\n","For example, the average age of a person considered in the study is 55 years, where the minimum age is 29 years and the maximum age is 77 years.<br><br>\n","It can be seen that all the numeric variables have 303 observations which is equal to the total number of observations. This suggests that there are no missing values.</b>  \n","                    </font>\n","            </div>\n","        </td>\n","    </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"DUylzUstwww2"},"source":["**2. For categorical variables, use the describe(include=object)**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K7xasuY6eboF","scrolled":true},"outputs":[],"source":["# summary of the categorical variables \n","#df_features.describe(include = object)\n","#Transpose the result for better readability\n","df_features.describe(include='object').transpose()\n","# Note: if we pass 'include=object' to the .describe() function returns descriptive statistics for categorical variables only"]},{"cell_type":"markdown","metadata":{"id":"MO3EQF7DeboJ"},"source":["<table align=\"left\">\n","    <tr>\n","        <td width=\"8%\">\n","            <img src=\"infer.png\">\n","        </td>\n","        <td>\n","            <div align=\"left\", style=\"font-size:120%\">\n","                <font color=\"#21618C\">\n","<b>The summary contains information about the total number of observations, number of unique classes, the most occurring class and frequency of the same.\n","<br><br>\n","    Let us consider the 'rest_ecg' variable to see the statistics- <br>\n","count: Number of observations i.e., 303<br>\n","unique: Number of unique values or classes in the variable i.e., it has 3 classes in it namely, 0, 1 and 2<br>\n","top: The most occurring class in this variable is 1<br> \n","frequency: Frequency of the most repeated class; out of 303 observations 1 has a frequency of 152\n","                    </b>  \n","               </font>\n","            </div>\n","        </td>\n","    </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"OehAU0ZKwww3"},"source":["<a id='distribution_variables'></a>\n","### 4.1.3 Distribution of Variables"]},{"cell_type":"markdown","metadata":{"id":"p4EHAGXPwww3"},"source":["<table align=\"left\">\n","    <tr>\n","        <td width=\"8%\">\n","            <img src=\"todo.png\">\n","        </td>\n","        <td>\n","            <div align=\"left\", style=\"font-size:120%\">\n","                <font color=\"#21618C\">\n","                    <b>Check the distribution of all the variables <br><br>\n","                        1. Distribution of numeric independent variables<br>\n","                        2. Distribution of categoric independent variables<br>\n","                        3. Distribution of dependent variable\n","                    </b>\n","                </font>\n","            </div>\n","        </td>\n","    </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"-tWX9ptGeboe"},"source":["#### 1. Distribution of numeric independent variables."]},{"cell_type":"markdown","metadata":{"id":"ai5_xbHWwww4"},"source":["For the independent numeric variables, we plot the histogram to check the distribution of the variables."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ne5rQHn5ebof","scrolled":false},"outputs":[],"source":["# plot the histogram of numeric independent variables\n","# the hist() function considers the numeric variables only, by default\n","df_features.hist()\n","\n","# adjust the subplots\n","plt.tight_layout()\n","\n","# display the plot\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"RZScN7oIwww4"},"source":["<table align=\"left\">\n","    <tr>\n","        <td width=\"8%\">\n","            <img src=\"infer.png\">\n","        </td>\n","        <td>\n","            <div align=\"left\", style=\"font-size:120%\">\n","                <font color=\"#21618C\">\n","                    <b> It can be seen that the variable 'old_peak' is right-skewed and variables 'cholestrol' and 'rest_bps' have slight right skew, but not a very long right tail. They are almost near normally distrubuted. Only the variable 'thalach' is slightly left-skewed. <br> <br> \n","                        The variable which is near normally distributed is 'age'.<br><br>  \n","                     </b>\n","                </font>\n","            </div>\n","        </td>\n","    </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"v1hJh7QHebow"},"source":["#### 2. Distribution of categoric independent variables."]},{"cell_type":"markdown","metadata":{"id":"DLyv4ii9www4"},"source":["For the independent categoric variables, we plot the bar plot to check the distribution of each variables."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G7HE9bl9eboz"},"outputs":[],"source":["# create a list of all categorical variables\n","# initiate an empty list to store the categorical variables\n","categorical=[]\n","\n","# use for loop to check the data type of each variable\n","for column in df_features:\n","    \n","    # use 'if' statement with condition to check the categorical type \n","    if is_string_dtype(df_features[column]):\n","        \n","        # append the variables with 'categoric' data type in the list 'categorical'\n","        categorical.append(column)\n","\n","# plot the count plot for each categorical variable \n","# set the number of rows in the subplot using the parameter, 'nrows'\n","# set the number of columns in the subplot using the parameter, 'ncols'\n","# 'figsize' sets the figure size\n","fig, ax = plt.subplots(nrows = 3, ncols = 3, figsize=(25, 20))\n","\n","# use for loop to plot the count plot for each variable\n","for variable, subplot in zip(categorical, ax.flatten()):\n","    \n","    # use countplot() to plot the graph\n","    # pass the axes for the plot to the parameter, 'ax'\n","    sns.countplot(df_Heart[variable], ax = subplot)\n","\n","# display the plot\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"kzMnW3Ocwww5"},"source":["<table align=\"left\">\n","    <tr>\n","        <td width=\"8%\">\n","            <img src=\"infer.png\">\n","        </td>\n","        <td>\n","            <div align=\"left\", style=\"font-size:120%\">\n","                <font color=\"#21618C\">\n","                    <b> The variable 'ca' has 5 categories, variable 'chest_pain' and 'thalassemia' have 4 categories each, variables 'rest_ecg' and 'slope' have 3 categories each whereas the remaining variables have only two categories.</b>\n","                </font>\n","            </div>\n","        </td>\n","    </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"row0AGsuwww6"},"source":["#### 3. Distribution of dependent variable."]},{"cell_type":"markdown","metadata":{"id":"7XAqBmkawww6"},"source":["In section 4.1.1, we have split the dependent variable (target) and created a dataframe 'df_target'. Use this dataframe to check the distribution of target."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"M7lMgaWpwww6"},"outputs":[],"source":["# get counts of 0's and 1's in the 'target' variable using 'value_counts()'\n","# store the values in 'class_frequency'\n","class_frequency = df_target.target.value_counts()\n","class_frequency"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"STDtaBmgwww7"},"outputs":[],"source":["# plot the countplot of the variable 'target'\n","sns.countplot(x = df_target.target)\n","\n","# use below code to print the values in the graph\n","# 'x' and 'y' gives position of the text\n","# 's' is the text on the plot\n","plt.text(x = -0.05, y = df_target.target.value_counts()[0] + 30, s = str((class_frequency[0])*100/len(df_target.target)) + '%')\n","plt.text(x = 0.95, y = df_target.target.value_counts()[1] +20, s = str((class_frequency[1])*100/len(df_target.target)) + '%')\n","\n","# add plot and axes labels\n","# set text size using 'fontsize'\n","plt.title('Count Plot for Target Variable', fontsize = 15)\n","plt.xlabel('Target Variable', fontsize = 15)\n","plt.ylabel('Count', fontsize = 15)\n","\n","# to show the plot\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"lNxV4lFfwww7"},"source":["<table align=\"left\">\n","    <tr>\n","        <td width=\"8%\">\n","            <img src=\"infer.png\">\n","        </td>\n","        <td>\n","            <div align=\"left\", style=\"font-size:120%\">\n","                <font color=\"#21618C\">\n","                    <b>There are a total of 138 patients who do not have the disease, and 165 patients are affected by the Heart disease. We see that there is a balance between the two classes of the target variable.</b>\n","                </font>\n","            </div>\n","        </td>\n","    </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"vkZahcS9eboQ"},"source":["<a id='correlation'></a>\n","### 4.1.4 Correlation"]},{"cell_type":"markdown","metadata":{"id":"K_oo8YEAeboS"},"source":["Correlation is a statistic that measures the degree to which two variables move with each other. A correlation coefficient near  1  indicates the strong relationship between them; a weak correlation indicates the extent to which one variable increases as the other decreases. Correlation among multiple variables can be represented in the form of a matrix. This allows us to see which variables are correlated."]},{"cell_type":"markdown","metadata":{"id":"sAagGX-keboT"},"source":["<table align=\"left\">\n","    <tr>\n","        <td width=\"8%\">\n","            <img src=\"todo.png\">\n","        </td>\n","        <td>\n","            <div align=\"left\", style=\"font-size:120%\">\n","                <font color=\"#21618C\">\n","                    <b> To check the correlation between numerical variables, we perform the following steps:<br><br>\n","                    1. Compute a correlation matrix  <br>\n","                    2. Plot a heatmap for the correlation matrix\n","                    </b>\n","                </font>\n","            </div>\n","        </td>\n","    </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"K4b-vSDZwww8"},"source":["**1. Compute a correlation matrix**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_eqWwTr8eboZ"},"outputs":[],"source":["# use the corr() function to generate the correlation matrix of the numeric variables\n","corr = df_features.corr()\n","\n","# print the correlation matrix\n","corr"]},{"cell_type":"markdown","metadata":{"id":"XB3gxvJewww8"},"source":["**2. Plot the heatmap for the diagonal correlation matrix**"]},{"cell_type":"markdown","metadata":{"id":"4ShmM0sRwww9"},"source":["A correlation matrix is a symmetric matrix. Plot only the lower triangular entries using a heatmap."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oeXcIgFyebob","scrolled":false},"outputs":[],"source":["# use 'mask' to plot a lower triangular correlation matrix \n","# 'triu_indices_from' returns the indices for the upper-triangle of matrix\n","mask = np.zeros_like(corr)\n","mask[np.triu_indices_from(mask, k=1)] = True\n","\n","# plot the heat map\n","# corr: give the correlation matrix\n","# cmap: color code used for plotting\n","# vmax: gives a maximum range of values for the chart\n","# vmin: gives a minimum range of values for the chart\n","# annot: prints the correlation values in the chart\n","# annot_kws: sets the font size of the annotation\n","# mask: mask the upper traingular matrix values\n","sns.heatmap(corr, cmap = 'YlGnBu', vmax = 1.0, vmin = -1.0, annot = True, annot_kws = {\"size\": 12}, mask = mask)\n","\n","# display the plot\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"DiZ_ts_Debod"},"source":["<table align=\"left\">\n","    <tr>\n","        <td width=\"8%\">\n","            <img src=\"infer.png\">\n","        </td>\n","        <td>\n","            <div align=\"left\", style=\"font-size:120%\">\n","                <font color=\"#21618C\">\n","                    <b>From the above heatmap, it can seen that the continuous variables are not highly correlated with each other since all the correlation coefficients are less than close to 0.5. The maximum positive correlation is 0.28 between 'age' and 'rest_bps'. Variable 'thalach' has a moderate negative correlation with variables 'age' (-0.4) and 'old_peak'(-0.34).  It can be concluded that there is no multicollinearity in the data.<br><br>\n","Note: The diagonal values are always 1 because it is the correlation of the variable with itself.</b>\n","                </font>\n","            </div>\n","        </td>\n","    </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"pPwoQF_Hebod"},"source":["<table align=\"left\">\n","    <tr>\n","        <td width=\"8%\">\n","            <img src=\"quicktip.png\">\n","        </td>\n","        <td>\n","            <div align=\"left\", style=\"font-size:120%\">\n","                <font color=\"#21618C\">\n","                    <b>Correlation does not imply causation. In other words, if two variables are correlated, it does not mean that one variable caused the other.</b>\n","                </font>\n","            </div>\n","        </td>\n","    </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"OET43huSeboe"},"source":["<table align=\"left\">\n","    <tr>\n","        <td width=\"8%\">\n","            <img src=\"alsoreadicon.png\">\n","        </td>\n","        <td>\n","            <div align=\"left\", style=\"font-size:120%\">\n","                <font color=\"#21618C\">\n","                    <b>I love to know more:</b> <br><br>\n","                    <a href=\"https://bit.ly/2PBvA8T\">Why correlation does not imply causation </a>\n","                </font>\n","            </div>\n","        </td>\n","    </tr>\n","</table>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"FjtDatMReboh"},"source":["<a id='outliers'></a>\n","### 4.1.5 Discover Outliers "]},{"cell_type":"markdown","metadata":{"id":"rv_JBRXxeboi"},"source":["#### Importance of detecting an outlier\n","An outlier is an observation that appears to deviate distinctly from other observations in the data. If the outliers are not removed, the model accuracy may decrease."]},{"cell_type":"markdown","metadata":{"id":"zMU5iU17www-"},"source":["<table align=\"left\">\n","    <tr>\n","        <td width=\"8%\">\n","            <img src=\"todo.png\">\n","        </td>\n","        <td>\n","            <div align=\"left\", style=\"font-size:120%\">\n","                <font color=\"#21618C\">\n","                    <b>To detect outliers in numeric data, we perform the following:<br><br>\n","                    1. Plot the boxplot for numeric data<br>\n","                    2. Note the variables in which outliers are present<br>\n","                    3. Remove outliers by IQR method<br> \n","                    4. Plot the boxplot to recheck for outliers</b>\n","                </font>\n","            </div>\n","        </td>\n","    </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"miAEjo_hwww-"},"source":["**1. Plot the boxplot for numeric data**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WGaqwSezeboj"},"outputs":[],"source":["# plot a boxplot to visualize the outliers in all the numeric variables\n","df_features.boxplot()\n","\n","# set plot label\n","# set text size using 'fontsize'\n","plt.title('Distribution of all Numeric Variables', fontsize = 15)\n","\n","# xticks() returns the x-axis ticks\n","# 'rotation = vertical' rotates the x-axis labels vertically\n","plt.xticks(rotation = 'vertical', fontsize = 15)\n","\n","# display the plot\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"--zciCKbwww_"},"source":["**Notice that the variables 'age' , 'old_peak' has a quite small range as compared to the other variables. Thus, it is difficult to see the outliers for these variables. So, we plot the boxplot only for the variables 'age', 'old_peak'.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uA4Tsm6Hwww_"},"outputs":[],"source":["# make a list of numerical features \n","cols = [ 'age','old_peak']\n","\n","# plot multiple boxplots\n","df_features[cols].boxplot()\n","\n","# set plot label\n","# set text size using 'fontsize'\n","plt.title('Distribution of Independent Variables age and old_peak', fontsize = 15)\n","\n","# xticks() returns the x-axis ticks\n","# 'rotation = vertical' rotates the x-axis labels vertically\n","plt.xticks(rotation = 'vertical', fontsize = 15)\n","\n","# display the plot\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"Vz3aKA6Rwww_"},"source":["**2. Note the variables for which outliers are present**"]},{"cell_type":"markdown","metadata":{"id":"AXWCnG8yebol"},"source":["<table align=\"left\">\n","    <tr>\n","        <td width=\"8%\">\n","            <img src=\"infer.png\">\n","        </td>\n","        <td>\n","            <div align=\"left\", style=\"font-size:120%\">\n","                <font color=\"#21618C\">\n","                    <b>From the above plot, we notice that for the variables 'rest_bps', 'cholestrol','thalach' and 'old_peak' there are points above the upper extreme or below the lower extreme - these points are outliers in the data. However, the exact location of these points is not precisely seen.<br><br>\n","                        Let us use the IQR method to remove the outliers.<br><br>\n","                        Note: The variables have not been scaled. Thus the boxplots are not visualized efficiently.</b>\n","                </font>\n","            </div>\n","        </td>\n","    </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"gqK11WzYebom"},"source":["<table align=\"left\">\n","    <tr>\n","        <td width=\"8%\">\n","            <img src=\"quicktip.png\">\n","        </td>\n","        <td>\n","            <div align=\"left\", style=\"font-size:120%\">\n","                <font color=\"#21618C\">\n","                    <b>Outliers can also be detected using the standard deviation method if we know that the distribution of values in the sample is Gaussian or Gaussian-like.</b> \n","                </font>\n","            </div>\n","        </td>\n","    </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"BudwXJ6Kebou"},"source":["<table align=\"left\">\n","    <tr>\n","        <td width=\"8%\">\n","            <img src=\"alsoreadicon.png\">\n","        </td>\n","        <td>\n","            <div align=\"left\", style=\"font-size:120%\">\n","                <font color=\"#21618C\">\n","                    <b>I love to know more:  </b><a href=\"https://bit.ly/33bgNpq\">How to use statistics to identify outliers in data</a>\n","</font>\n","            </div>\n","        </td>\n","    </tr>\n","</table>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"3etMxGumwwxA"},"source":["**3. Remove outliers by IQR method**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HBc-FXUpebom"},"outputs":[],"source":["# calculate the first quartile\n","Q1 = df_features.quantile(0.25)\n","\n","# calculate the third quartile\n","Q3 = df_features.quantile(0.75)\n","\n","# Interquartile Range (IQR) is defined as the difference between the third and first quartile\n","# calculate IQR\n","IQR = Q3 - Q1\n","\n","# print the IQR\n","print(IQR)"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"OGb5EZBpebop"},"outputs":[],"source":["# remove the outliers from the dataframe 'df_Heart'\n","# retrieve the dataframe without the outliers\n","# '~' returns the values that do not satisfy the given conditions \n","# i.e. it returns values between the range [Q1-1.5*IQR, Q3+1.5*IQR]\n","# '|' is used as 'OR' operator on multiple conditions   \n","# 'any(axis=1)' checks the entire row for atleast one 'True' entry (those rows represents outliers in the data)\n","df_Heart = df_Heart[~((df_Heart < (Q1 - 1.5 * IQR)) | (df_Heart > (Q3 + 1.5 * IQR))).any(axis=1)]\n","\n","# reset the index of the dataframe without outliers\n","df_Heart = df_Heart.reset_index(drop = True)"]},{"cell_type":"markdown","metadata":{"id":"4iuSrkGhebor"},"source":["To confirm that the outliers have been removed; let us visualize the boxplot again."]},{"cell_type":"markdown","metadata":{"id":"pyt7WYCpwwxB"},"source":["**4. Plot the boxplot to recheck for outliers**"]},{"cell_type":"markdown","metadata":{"id":"T4dRHGd1wwxB"},"source":["We plot the boxplots for all variables except for the variable `white corpuscle` for better visualization."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2ZmatCI-ebos"},"outputs":[],"source":["# make a list of numerical features without considering the 'white corpuscle'\n","cols = ['age','rest_bps','cholestrol','thalach','old_peak']\n","\n","# plot multiple boxplots\n","df_Heart[cols].boxplot()\n","\n","# set plot label\n","# set text size using 'fontsize'\n","plt.title('Distribution of Independent Variables', fontsize = 15)\n","\n","# xticks() returns the x-axis ticks\n","# 'rotation = vertical' rotates the x-axis labels vertically\n","plt.xticks(rotation = 'vertical', fontsize = 15)\n","\n","# display the plot\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"w_koYPUawwxB"},"source":["Observing the range of the boxplot, we say that the outliers are removed from the original data.\n","\n","It is up to the discretion of the data scientist, to remove them or not; and maybe decide after evaluating the model performance. "]},{"cell_type":"markdown","metadata":{"id":"sAAk9vaIwwxB"},"source":["A crude way to know whether the outliers have been removed or not is to check the dimensions of the data. If the dimensions are reduced that implies outliers are removed."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ERHiDR7tebov"},"outputs":[],"source":["# check the shape of the data after removal of outliers \n","df_Heart.shape"]},{"cell_type":"markdown","metadata":{"id":"XkBHFTwKwwxC"},"source":["<table align=\"left\">\n","    <tr>\n","        <td width=\"8%\">\n","           <img src=\"infer.png\">\n","        </td>\n","        <td>\n","            <div align=\"left\", style=\"font-size:120%\">\n","                <font color=\"#21618C\">\n","                    <b>The output shows a reduction in the number of rows. Thus we may say that the potential outliers have been removed.</b>\n","                </font>\n","            </div>\n","        </td>\n","    </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"G-aTB0DseboJ"},"source":["<a id='Missing_Values'></a>\n","### 4.1.6 Missing Values"]},{"cell_type":"markdown","metadata":{"id":"OKJX5vi4wwxC"},"source":["<table align=\"left\">\n","    <tr>\n","        <td width=\"8%\">\n","            <img src=\"todo.png\">\n","        </td>\n","        <td>\n","            <div align=\"left\", style=\"font-size:120%\">\n","                <font color=\"#21618C\">\n","                    <b>First, run a check for the presence of missing values and their percentage for each column. Then choose the right approach to remove them.</b>\n","                </font>\n","            </div>\n","        </td>\n","    </tr>\n","</table>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XsY-ydhgeboM"},"outputs":[],"source":["# sort the variables on the basis of total null values in the variable\n","# 'isnull().sum()' returns the number of missing values in each variable\n","# 'ascending = False' sorts values in the descending order\n","# the variable with highest number of missing values will appear first\n","Total = df_Heart.isnull().sum().sort_values(ascending = False)          \n","\n","# calculate the percentage of missing values\n","# 'ascending = False' sorts values in the descending order\n","# the variable with highest percentage of missing values will appear first\n","Percent = (df_Heart.isnull().sum()*100/df_Heart.isnull().count()).sort_values(ascending = False)   \n","\n","# concat the 'Total' and 'Percent' columns using 'concat' function\n","# 'keys' is the list of column names\n","# 'axis = 1' concats along the columns\n","missing_data = pd.concat([Total, Percent], axis = 1, keys = ['Total', 'Percentage of Missing Values'])    \n","missing_data"]},{"cell_type":"markdown","metadata":{"id":"qstii9FGwwxD"},"source":["<table align=\"left\">\n","    <tr>\n","        <td width=\"8%\">\n","            <img src=\"infer.png\">\n","        </td>\n","        <td>\n","            <div align=\"left\", style=\"font-size:120%\">\n","                <font color=\"#21618C\">\n","                    <b>None of the variables contain the missing values.</b>\n","                </font>\n","            </div>\n","        </td>\n","    </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"1zh6xtCUwwxD"},"source":["Another way to find the missing values is to plot a heatmap for visualization."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Or14vR2EwwxD"},"outputs":[],"source":["# plot heatmap to check null values\n","# 'cbar = False' does not show the color axis \n","sns.heatmap(df_Heart.isnull(), cbar=False)\n","\n","# display the plot\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"9hvE0dwNwwxD"},"source":["There are no horizontal lines in the heatmap which would correspond to a probable missing values."]},{"cell_type":"markdown","metadata":{"id":"Dogz55FZeboP"},"source":["<table align=\"left\">\n","    <tr>\n","        <td width=\"8%\">\n","            <img src=\"infer.png\">\n","        </td>\n","        <td>\n","            <div align=\"left\", style=\"font-size:120%\">\n","                <font color=\"#21618C\">\n","                    <b> The above output shows that there are no missing values in the data.</b>\n","                </font>\n","            </div>\n","        </td>\n","    </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"bEFjxuUswwxE"},"source":["<table align=\"left\">\n","    <tr>\n","        <td width=\"8%\">\n","            <img src=\"quicktip.png\">\n","        </td>\n","        <td>\n","            <div align=\"left\", style=\"font-size:120%\">\n","                <font color=\"#21618C\">\n","                    <b>How to deal with missing data?<br><br>\n","\n","\n","Drop data<br>\n","a. Drop the whole row<br>\n","b. Drop the whole column<br><br>\n","\n","Replace data<br>\n","\n","a. Replace it by mean<br>\n","\n","b. Replace it by frequency<br>\n","c. Replace it based on other functions<br><br>\n","\n","Whole columns should be dropped only if most entries in the column are empty. In our dataset, none of the columns are empty enough to drop entirely. We have some freedom in choosing the method to replace the data.</b>\n","                </font>\n","            </div>\n","        </td>\n","    </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"a8EfOhNXwwxE"},"source":["<a id='Data_Preparation'></a>\n","## 4.2 Prepare the Data"]},{"cell_type":"markdown","metadata":{"id":"IKcD70E9wwxE"},"source":["To build the classification models, we need to encode the categorical variables using dummy encoding."]},{"cell_type":"markdown","metadata":{"id":"cJCv7mP3wwxE"},"source":["<table align=\"left\">\n","    <tr>\n","        <td width=\"8%\">\n","            <img src=\"todo.png\">\n","        </td>\n","        <td>\n","            <div align=\"left\", style=\"font-size:120%\">\n","                <font color=\"#21618C\">\n","                    <b> \n","                  To dummy encode, we do the following: <br><br>\n","                    1. Filter numerical and categorical variables<br>\n","                    2. Dummy encode the categorical variables<br>\n","                    3. Concatenate numerical and dummy encoded categorical variables</b>\n","                </font>\n","            </div>\n","        </td>\n","    </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"wA5hnr6NwwxF"},"source":["**1. Filter numerical and categorical variables **"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"DLtmzY72eboU"},"outputs":[],"source":["# dataframe with categorical features\n","# 'categorical' contains a list of categorical variables\n","df_cat = df_Heart[categorical]\n","\n","# dataframe with numerical features\n","# use 'drop()' to drop the categorical variables\n","# 'axis = 1' drops the corresponding column(s)\n","df_num = df_Heart.drop(categorical, axis = 1)"]},{"cell_type":"markdown","metadata":{"id":"rgrynyk-wwxF"},"source":["**2. Dummy encode the categorical variables**"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"gzj44_nDwwxF"},"outputs":[],"source":["# print the first five observations of the 'df_cat'\n","df_cat.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"rLNKCK7lebo3"},"outputs":[],"source":["# use 'get_dummies()' from pandas to create dummy variables\n","# use 'drop_first = True' to create (n-1) dummy variables\n","df_cat_dummies = pd.get_dummies(df_cat, drop_first = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PYPnUX59wwxG"},"outputs":[],"source":["# check the first five observations of the data with dummy encoded variables\n","df_cat_dummies.head()"]},{"cell_type":"markdown","metadata":{"id":"6xmta2gcwwxG"},"source":["**3. Concatenate numerical and dummy encoded categorical variables**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U90M-Hokebo7"},"outputs":[],"source":["# concat the dummy variables with numeric features to create a dataframe of all independent variables\n","# 'axis=1' concats the dataframes along columns \n","df_Heart_dummy = pd.concat([df_num, df_cat_dummies], axis=1)\n","\n","# display first five observations of the dummy dataframe\n","df_Heart_dummy.head()"]},{"cell_type":"markdown","metadata":{"id":"rrfTrS4BwwxG"},"source":["After removal of outliers and missing values in the data, the dataframe `df_Heart_dummy` contains independent as well as dependent variables."]},{"cell_type":"markdown","metadata":{"id":"SJKhboslwwxG"},"source":["<table align=\"left\">\n","    <tr>\n","        <td width=\"8%\">\n","            <img src=\"todo.png\">\n","        </td>\n","        <td>\n","            <div align=\"left\", style=\"font-size:120%\">\n","                <font color=\"#21618C\">\n","                    <b>Split the dependent variable (target) from the dataframe 'df_Heart_dummy'.</b>\n","                </font>\n","            </div>\n","        </td>\n","    </tr>\n","</table>"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"IBKWuemowwxH"},"outputs":[],"source":["# select only the target variable 'target' and store it in dataframe 'y'\n","y = pd.DataFrame(df_Heart_dummy['target'])"]},{"cell_type":"markdown","metadata":{"id":"dlmnRIfAwwxH"},"source":["Now, use this 'y' as a target variable to build the classification models."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"b5jHXkbdwwxH"},"outputs":[],"source":["# use 'drop()' to remove the variable 'target' from df_target_dummy\n","# 'axis = 1' drops the corresponding column(s)\n","X = df_Heart_dummy.drop('target',axis = 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UQstRNsZwwxH"},"outputs":[],"source":["# check the first five observations of X\n","X.head()"]},{"cell_type":"markdown","metadata":{"id":"nJ0p99GwwwxI"},"source":["Use this 'X' as a set of predictors to build the classification models."]},{"cell_type":"markdown","metadata":{"id":"iAUZmXiwwwxI"},"source":["#### Create a generalized function to calculate the metrics for the test set."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"Cefb3fdcwwxI"},"outputs":[],"source":["# create a generalized function to calculate the metrics values for test set\n","def get_test_report(model):\n","    \n","    # return the performace measures on test set\n","    return(classification_report(y_test, y_pred))"]},{"cell_type":"markdown","metadata":{"id":"vjRIIkWnwwxI"},"source":["#### Create a generalized function to calculate the kappa score for the test set."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"YcuWTju0wwxJ"},"outputs":[],"source":["# create a generalized function to calculate the metrics values for test set\n","def kappa_score(model):\n","    \n","    # return the kappa score on test set\n","    return(cohen_kappa_score(y_test, y_pred))"]},{"cell_type":"markdown","metadata":{"id":"Ly5aN_X8wwxJ"},"source":["#### Define a function to plot the confusion matrix."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"Av39nL7SebpS"},"outputs":[],"source":["# define a to plot a confusion matrix for the model\n","def plot_confusion_matrix(model):\n","    \n","    # create a confusion matrix\n","    # pass the actual and predicted target values to the confusion_matrix()\n","    cm = confusion_matrix(y_test, y_pred)\n","\n","    # label the confusion matrix  \n","    # pass the matrix as 'data'\n","    # pass the required column names to the parameter, 'columns'\n","    # pass the required row names to the parameter, 'index'\n","    conf_matrix = pd.DataFrame(data = cm,columns = ['Predicted:0','Predicted:1'], index = ['Actual:0','Actual:1'])\n","\n","    # plot a heatmap to visualize the confusion matrix\n","    # 'annot' prints the value of each grid \n","    # 'fmt = d' returns the integer value in each grid\n","    # 'cmap' assigns color to each grid\n","    # as we do not require different colors for each grid in the heatmap,\n","    # use 'ListedColormap' to assign the specified color to the grid\n","    # 'cbar = False' will not return the color bar to the right side of the heatmap\n","    # 'linewidths' assigns the width to the line that divides each grid\n","    # 'annot_kws = {'size':25})' assigns the font size of the annotated text \n","    sns.heatmap(conf_matrix, annot = True, fmt = 'd', cmap = ListedColormap(['lightskyblue']), cbar = False, \n","                linewidths = 0.1, annot_kws = {'size':25})\n","\n","    # set the font size of x-axis ticks using 'fontsize'\n","    plt.xticks(fontsize = 20)\n","\n","    # set the font size of y-axis ticks using 'fontsize'\n","    plt.yticks(fontsize = 20)\n","\n","    # display the plot\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"7id6ZNGawwxK"},"source":["#### Define a function to plot the ROC curve."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"FTt7Jkd6wwxK"},"outputs":[],"source":["# define a function to plot the ROC curve and print the ROC-AUC score\n","def plot_roc(model):\n","    \n","    # the roc_curve() returns the values for false positive rate, true positive rate and threshold\n","    # pass the actual target values and predicted probabilities to the function\n","    fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n","\n","    # plot the ROC curve\n","    plt.plot(fpr, tpr)\n","\n","    # set limits for x and y axes\n","    plt.xlim([0.0, 1.0])\n","    plt.ylim([0.0, 1.0])\n","\n","    # plot the straight line showing worst prediction for the model\n","    plt.plot([0, 1], [0, 1],'r--')\n","\n","    # add plot and axes labels\n","    # set text size using 'fontsize'\n","    plt.title('ROC Curve for Heart Disease Classifier', fontsize = 15)\n","    plt.xlabel('False positive rate (1-Specificity)', fontsize = 15)\n","    plt.ylabel('True positive rate (Sensitivity)', fontsize = 15)\n","\n","    # add the AUC score to the plot\n","    # 'x' and 'y' gives position of the text\n","    # 's' is the text \n","    # use round() to round-off the AUC score upto 4 digits\n","    plt.text(x = 0.02, y = 0.9, s = ('AUC Score:',round(roc_auc_score(y_test, y_pred_prob),4)))\n","\n","    # plot the grid\n","    plt.grid(True)"]},{"cell_type":"markdown","metadata":{"id":"jwNiU2k-wwxK"},"source":["#### Create a generalized function to create a dataframe containing the scores for the models."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"OpKZly82wwxL"},"outputs":[],"source":["# create an empty dataframe to store the scores for various classification algorithms\n","score_card = pd.DataFrame(columns=['Model', 'AUC Score', 'Precision Score', 'Recall Score', 'Accuracy Score',\n","                                   'Kappa Score', 'f1-score'])\n","\n","# append the result table for all performance scores\n","# performance measures considered for comparision are 'AUC', 'Precision', 'Recall','Accuracy','Kappa Score', and 'f1-score'\n","# compile the required information in a user defined function \n","def update_score_card(model_name):\n","    \n","    # assign 'score_card' as global variable\n","    global score_card\n","\n","    # append the results to the dataframe 'score_card'\n","    # 'ignore_index = True' do not consider the index labels\n","    score_card = score_card.append({'Model': model_name,\n","                                    'AUC Score' : roc_auc_score(y_test, y_pred_prob),\n","                                    'Precision Score': metrics.precision_score(y_test, y_pred),\n","                                    'Recall Score': metrics.recall_score(y_test, y_pred),\n","                                    'Accuracy Score': metrics.accuracy_score(y_test, y_pred),\n","                                    'Kappa Score': cohen_kappa_score(y_test, y_pred),\n","                                    'f1-score': metrics.f1_score(y_test, y_pred)}, \n","                                    ignore_index = True)\n","    return(score_card)"]},{"cell_type":"markdown","metadata":{"id":"Nvj6SJi9ebqY"},"source":["<a id='DecisionTree'> </a>\n","# 5. Decision Tree"]},{"cell_type":"markdown","metadata":{"id":"YnPBtZ5jwwxL"},"source":["Decision Tree is a non-parametric supervised learning method. It builds a regression model in the form of a tree structure. It breaks down a data set into smaller and smaller subsets, which is called splitting. The final result is a tree with a decision and leaf nodes. A decision node has two or more branches. The leaf node represents a class or decision. The topmost decision node in a tree that corresponds to the best predictor called 'root node'. The decision tree is built using different criteria like gini index, and entropy. "]},{"cell_type":"markdown","metadata":{"id":"4Qsoj-okwwxM"},"source":[" \n","<table align=\"left\">\n","    <tr>\n","        <td width=\"8%\">\n","            <img src=\"alsoreadicon.png\">\n","        </td>\n","        <td>\n","            <div align=\"left\", style=\"font-size:120%\">\n","                <font color=\"#21618C\">\n","                    <b> I love to know more:<br>\n","                        1. <a href=\"https://bit.ly/3534vQ7\"> Understanding Decision Trees</a></b>\n","                </font>\n","            </div>\n","        </td>\n","    </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"jZRHUDo3wwxM"},"source":["<a id='DecisionTreeWFS'> </a>\n","## 5.1 Decision Tree "]},{"cell_type":"markdown","metadata":{"id":"64CdCBMtwwxM"},"source":["<table align=\"left\">\n","    <tr>\n","        <td width=\"8%\">\n","            <img src=\"todo.png\">\n","        </td>\n","        <td>\n","            <div align=\"left\", style=\"font-size:120%\">\n","                <font color=\"#21618C\">\n","                    <b>To build a Decision Tree, we do the following: <br><br>\n","                       1. Split the data into training and test sets<br> \n","                       2. Build the model <br>\n","                       3. Plot the decision tree<br>\n","                       4. Do predictions on the test set <br>\n","                       5. Compute accuracy measures <br>\n","                       6. Tabulate the results\n","</b>\n","                </font>\n","            </div>\n","        </td>\n","    </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"k0q_T_eEwwxM"},"source":["**1. Split the data into training and test sets**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"01Bw0doXwwxM"},"outputs":[],"source":["# split data into train subset and test subset\n","# set 'random_state' to generate the same dataset each time you run the code \n","# 'test_size' returns the proportion of data to be included in the test set\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 10)\n","\n","# check the dimensions of the train & test subset using 'shape'\n","# print dimension of train set\n","print(\"X_train\",X_train.shape)\n","print(\"y_train\",y_train.shape)\n","\n","# print dimension of test set\n","print(\"X_test\",X_test.shape)\n","print(\"y_test\",y_test.shape)"]},{"cell_type":"markdown","metadata":{"id":"Lg6xztN7wwxN"},"source":["**2. Build the model**"]},{"cell_type":"markdown","metadata":{"id":"Zr_5a_MZwwxN"},"source":["<table align=\"left\">\n","    <tr>\n","        <td width=\"8%\">\n","            <img src=\"quicktip.png\">\n","        </td>\n","        <td>\n","            <div align=\"left\", style=\"font-size:120%\">\n","                <font color=\"#21618C\">\n","                    <b>To build the decision tree, we used the criterion of 'entropy'. Entropy is one of the criteria used to build the decision tree. It calculates the homogeneity of the sample. The entropy is zero if the sample is completely homogeneous, and it is equal to 1 if the sample is equally divided.\n","</b> \n","                </font>\n","            </div>\n","        </td>\n","    </tr>\n","</table>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"kRgAI_4uwwxN"},"source":["We build the decision tree on the `unscaled features`."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"r5SNCxwgebqY"},"outputs":[],"source":["# instantiate the 'DecisionTreeClassifier' object using 'entropy' criterion\n","# pass the 'random_state' to obtain the same samples for each time you run the code\n","decision_tree = DecisionTreeClassifier(criterion = 'entropy', random_state = 10)\n","\n","# fit the model using fit() on train data\n","decision_tree_model = decision_tree.fit(X_train, y_train)"]},{"cell_type":"markdown","metadata":{"id":"wlZkMH_ewwxN"},"source":["#### 3. Plot the decision tree"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KhSS6djUwwxO"},"outputs":[],"source":["# save the column names in 'labels'\n","lables = X_train.columns\n","\n","# plot the decision tree \n","fig = plt.figure(figsize=(30,30))\n","_ = tree.plot_tree(decision_tree_model, \n","                   feature_names=lables,  \n","                   class_names=[\"0\",\"1\"],\n","                   filled=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SXhn4AOHwwxO"},"outputs":[],"source":["# DOT data\n","dot_data = tree.export_graphviz(decision_tree_model, out_file=None, \n","                                feature_names=lables,  \n","                                class_names=[\"0\",\"1\"],\n","                                filled=True)\n","\n","# Draw graph\n","graph = graphviz.Source(dot_data, format=\"png\") \n","graph\n"]},{"cell_type":"markdown","metadata":{"id":"rLAT7fyvwwxO"},"source":["**4. Do predictions on the test set**"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"nPwP2i13wwxO"},"outputs":[],"source":["# predict probabilities on the test set\n","# consider the probability of positive class by subsetting with '[:,1]'\n","y_pred_prob = decision_tree_model.predict_proba(X_test)[:,1]"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"uvQXQNGlwwxP"},"outputs":[],"source":["# predict the class labels using 'X_test'\n","y_pred = decision_tree_model.predict(X_test)"]},{"cell_type":"markdown","metadata":{"id":"M3-X27qPwwxP"},"source":["**5. Compute accuracy measures**"]},{"cell_type":"markdown","metadata":{"id":"ytWNel1IwwxP"},"source":["#### Build a confusion matrix."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Afq7bBc8wwxP"},"outputs":[],"source":["# call the function to plot the confusion matrix\n","# pass the decision tree model to the function\n","plot_confusion_matrix(decision_tree_model)"]},{"cell_type":"markdown","metadata":{"id":"lqUhFKCJwwxP"},"source":["**Calculate performance measures on the test set.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UP5DA9_qebpC","scrolled":false},"outputs":[],"source":["# compute the performance measures on test data\n","# call the function 'get_test_report'\n","# pass the decision tree model to the function\n","test_report = get_test_report(decision_tree_model)\n","\n","# print the performace measures\n","print(test_report)"]},{"cell_type":"markdown","metadata":{"id":"F2GNJgOfwwxQ"},"source":["**Interpretation:** The accuracy is 71% for this model. Also, the sensitivity and specificity of the model is quite different."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DLK9yZeMwwxQ"},"outputs":[],"source":["# compute kappa score on test set\n","# call the function 'kappa_score'\n","# pass the decision tree model to the function\n","kappa_value = kappa_score(decision_tree_model)\n","\n","# print the kappa value\n","print(kappa_value)"]},{"cell_type":"markdown","metadata":{"id":"8q7Sh-wbwwxQ"},"source":["**Interpretation:** As the kappa score for the decision tree (pruned) is 0.4198, we can say that there is moderate agreement between the actual and predicted values."]},{"cell_type":"markdown","metadata":{"id":"LHRWRWuHwwxQ"},"source":["**Plot the ROC curve.**"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"Xsti84LowwxQ"},"outputs":[],"source":["# call the function 'plot_roc' to plot the ROC curve\n","# pass the decision tree model to the function\n","plot_roc(decision_tree_model)"]},{"cell_type":"markdown","metadata":{"id":"YDMm9aFkwwxR"},"source":["<table align=\"left\">\n","    <tr>\n","        <td width=\"8%\">\n","            <img src=\"infer.png\">\n","        </td>\n","        <td>\n","            <div align=\"left\", style=\"font-size:120%\">\n","                <font color=\"#21618C\">\n","                    <b>The dotted line represents the ROC curve of a purely random classifier; a good classifier stays as far away from that line as possible (toward the top-left corner).<br><br>\n","We see our classifier (decision tree) to be away from the dotted line with the AUC score 0.7105.</b>\n","                </font>\n","            </div>\n","        </td>\n","    </tr>\n","</table>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"oDWRop8PwwxR"},"source":["**7. Tabulate the results**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lYjpYGbfwwxR"},"outputs":[],"source":["# use the function 'update_score_card' to store the performance measures\n","# pass the 'Decision Tree' as model name to the function\n","update_score_card(model_name = 'Decision Tree')"]},{"cell_type":"markdown","metadata":{"id":"EnnMZei4wwxR"},"source":["<a id='DecisionTreePruning'></a>\n","## 5.2 Prune a Decision Tree "]},{"cell_type":"markdown","metadata":{"id":"FislLAr0wwxR"},"source":["<table align=\"left\">\n","    <tr>\n","        <td width=\"8%\">\n","            <img src=\"alsoreadicon.png\">\n","        </td>\n","        <td>\n","            <div align=\"left\", style=\"font-size:120%\">\n","                <font color=\"#21618C\">\n","                    <b> Pruning reduces the size of the decision tree by removing the internal nodes of the tree. It decreases the complexity of the tree, and so improves the accuracy of prediction. It is useful because trees may fit the training data well, but may do a poor prediction on test data. A simpler tree often avoids over-fitting.\n","</b>\n","                </font>\n","            </div>\n","        </td>\n","    </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"7qqVcY9_wwxS"},"source":["<table align=\"left\">\n","    <tr>\n","        <td width=\"8%\">\n","            <img src=\"todo.png\">\n","        </td>\n","        <td>\n","            <div align=\"left\", style=\"font-size:120%\">\n","                <font color=\"#21618C\">\n","                    <b> Now prune the decision tree, we start with our original data set gradually proceeding with our analysis<br><br>\n","                        To prune a 'Decision Tree', we do the following:<br><br>\n","                        1. Prune the decision tree <br>\n","                        2. Plot the decision tree<br>\n","                        3. Do predictions on the test set <br>\n","                        4. Compute accuracy measures <br>\n","                        5. Tabulate the results \n","                      </b>\n","                </font>\n","            </div>\n","        </td>\n","    </tr>\n","</table>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"CP6RFDOOwwxS"},"source":["**1. Prune the decision tree**"]},{"cell_type":"markdown","metadata":{"id":"HMKyTF8LwwxS"},"source":["We prune the decision tree by specifying the maximum depth and maximum number of leaves of the tree. \n","\n","We use the unscaled features to build the tree."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"IMjieFzEwwxS"},"outputs":[],"source":["# instantiate the 'DecisionTreeClassifier' object\n","# max_depth: maximum depth of the tree \n","# max_leaf_nodes: maximum number of leaf nodes in the tree\n","# pass the 'random_state' to obtain the same samples for each time you run the code\n","prune = DecisionTreeClassifier(max_depth = 5, max_leaf_nodes = 25 , random_state = 10)\n","\n","# fit the model using fit() on train data\n","decision_tree_prune = prune.fit(X_train, y_train)"]},{"cell_type":"markdown","metadata":{"id":"2TcgxfNewwxS"},"source":["#### 2. Plot the decision tree"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"Fp7sv8FuwwxT"},"outputs":[],"source":["# save the column names in 'labels'\n","lables = X_train.columns\n","\n","# plot the decision tree \n","fig = plt.figure(figsize=(30,30))\n","_ = tree.plot_tree(decision_tree_prune, \n","                   feature_names=lables,  \n","                   class_names=[\"0\",\"1\"],\n","                   filled=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pqW13lI4wwxT"},"outputs":[],"source":["# DOT data\n","dot_data = tree.export_graphviz(decision_tree_prune, out_file=None, \n","                                feature_names=lables,  \n","                                class_names=[\"0\",\"1\"],\n","                                filled=True)\n","\n","# Draw graph\n","graph = graphviz.Source(dot_data, format=\"png\") \n","graph\n"]},{"cell_type":"markdown","metadata":{"id":"4X-6ZrhawwxT"},"source":["**3. Do predictions on the test set**"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"GenDumH3wwxU"},"outputs":[],"source":["# predict probabilities on the test set\n","# consider the probability of positive class by subsetting with '[:,1]'\n","y_pred_prob = decision_tree_prune.predict_proba(X_test)[:,1]"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"snVfD_AywwxU"},"outputs":[],"source":["# predict the class labels using 'X_test'\n","y_pred = decision_tree_prune.predict(X_test)"]},{"cell_type":"markdown","metadata":{"id":"4A3MH6MDwwxU"},"source":["**4. Compute accuracy measures**"]},{"cell_type":"markdown","metadata":{"id":"KLuJWQA6wwxV"},"source":["#### Build a confusion matrix."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jnGRsR1twwxV"},"outputs":[],"source":["# call the function to plot the confusion matrix\n","# pass the decision tree (pruned) model to the function\n","plot_confusion_matrix(decision_tree_prune)"]},{"cell_type":"markdown","metadata":{"id":"eXCt-ev-wwxV"},"source":["**Calculate performance measures on the test set.**"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"zGRTmMdpwwxV"},"outputs":[],"source":["# compute the performance measures on test data\n","# call the function 'get_test_report'\n","# pass the decision tree (pruned) model to the function\n","test_report = get_test_report(decision_tree_prune)\n","\n","# print the performace measures\n","print(test_report)"]},{"cell_type":"markdown","metadata":{"id":"rLdP9gitwwxW"},"source":["**Interpretation:** The accuracy is 72% for this model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DNuPTbvowwxW"},"outputs":[],"source":["# compute kappa score on test set\n","# call the function 'kappa_score'\n","# pass the decision tree (pruned) model to the function\n","kappa_value = kappa_score(decision_tree_prune)\n","\n","# print the kappa value\n","print(kappa_value)"]},{"cell_type":"markdown","metadata":{"id":"LOVI2wW7wwxW"},"source":["**Interpretation:** As the kappa score for the decision tree (pruned) is 0.4421, we can say that there is moderate agreement between the actual and predicted values."]},{"cell_type":"markdown","metadata":{"id":"8xMF2834wwxX"},"source":["**Plot the ROC curve.**"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"6cSJmAQewwxX"},"outputs":[],"source":["# call the function 'plot_roc' to plot the ROC curve\n","# pass the decision tree (pruned) model to the function\n","plot_roc(decision_tree_prune)"]},{"cell_type":"markdown","metadata":{"id":"DBS7D3NTwwxX"},"source":["<table align=\"left\">\n","    <tr>\n","        <td width=\"8%\">\n","            <img src=\"infer.png\">\n","        </td>\n","        <td>\n","            <div align=\"left\", style=\"font-size:120%\">\n","                <font color=\"#21618C\">\n","                    <b>The dotted line represents the ROC curve of a purely random classifier; a good classifier stays as far away from that line as possible (toward the top-left corner).<br><br>\n","We see our classifier (decision tree with pruning) to be away from the dotted line with the AUC score 0.7389.</b>\n","                </font>\n","            </div>\n","        </td>\n","    </tr>\n","</table>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"uQgHT2OtwwxY"},"source":["**5. Tabulate the results**"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"jQ63P53AwwxY"},"outputs":[],"source":["# use the function 'update_score_card' to store the performance measures\n","# pass the 'Decision Tree (Pruned)' as model name to the function\n","update_score_card(model_name = 'Decision Tree (Pruned)')"]},{"cell_type":"markdown","metadata":{"id":"Vbs7RQIbebqn"},"source":["<a id='DecisionTreewithGridSearchCv'> </a>\n","## 5.3 Decision Tree (using GridSearchCV)"]},{"cell_type":"markdown","metadata":{"id":"NAI4j9eHwwxY"},"source":["Now we show how a decision tree is optimized by cross-validation, which is done using the `GridSearchCV()` from sklearn library.\n","\n","The performance of the selected hyperparameters and trained model is then measured on the test set that was not used during the model building."]},{"cell_type":"markdown","metadata":{"id":"42ER_m6KwwxZ"},"source":["<table align=\"left\">\n","    <tr>\n","        <td width=\"8%\">\n","            <img src=\"todo.png\">\n","        </td>\n","        <td>\n","            <div align=\"left\", style=\"font-size:120%\">\n","                <font color=\"#21618C\">\n","                    <b> Now we build a decision tree using the GridSearchCV. We start with our original data set gradually proceeding with our analysis<br><br>\n","                        To build a Decision Tree using GridSearchCV, we do the following:<br>\n","                        1. Use GridSearch to obtain the optimal values of hyperparameters <br>\n","                        2. Build the model using the hyperparameters obtained in step 1<br>\n","                        3. Plot the decision tree<br>\n","                        4. Do predictions on the test set <br>\n","                        5. Compute accuracy measures <br>\n","                        6. Tabulate the results\n","                      </b>\n","                </font>\n","            </div>\n","        </td>\n","    </tr>\n","</table>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"KuVwlH9qwwxZ"},"source":["**1. Use GridSearch to obtain the optimal values of hyperparameters**"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"N5olYIn_wwxZ"},"outputs":[],"source":["# create a dictionary with hyperparameters and its values\n","# pass the criteria 'entropy' and 'gini' to the parameter, 'criterion' \n","# pass the list of values to 'min_samples_split' that assigns minimum number of samples to split an internal node\n","# pass the list of values to 'max_depth' that assigns maximum depth of the tree\n","# pass the list of values to 'min_samples_leaf' that assigns minimum number of samples required at the terminal/leaf node\n","# pass the list of values to 'max_leaf_nodes' that assigns maximum number of leaf nodes in the tree\n","tuned_paramaters = [{'criterion': ['gini', 'entropy'],\n","                     'min_samples_split': [10, 20, 30],\n","                     'max_depth': [3, 5, 7, 9],\n","                     'min_samples_leaf': [15, 20, 25, 30, 35],\n","                     'max_leaf_nodes': [5, 10, 15, 20, 25]}]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XvMv41LOwwxa"},"outputs":[],"source":["# instantiate the 'DecisionTreeClassifier' \n","# pass the 'random_state' to obtain the same samples for each time you run the code\n","decision_tree_classification = DecisionTreeClassifier(random_state = 10)\n","\n","# use GridSearchCV() to find the optimal value of the hyperparameters\n","# estimator: pass the decision tree classifier model\n","# param_grid: pass the list 'tuned_parameters'\n","# cv: number of folds in k-fold i.e. here cv = 10\n","grid = GridSearchCV(estimator = decision_tree_classification, \n","                         param_grid = tuned_paramaters, \n","                         cv = 10)\n","\n","# fit the model on X_train and y_train using fit()\n","dt_grid = grid.fit(X_train, y_train)\n","\n","# get the best parameters\n","print('Best parameters for decision tree classifier: ', dt_grid.best_params_, '\\n')"]},{"cell_type":"markdown","metadata":{"id":"z4QpavaJwwxa"},"source":["**2. Build the model using the hyperparameters obtained in step 1**"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"hn5WcBvlwwxa"},"outputs":[],"source":["# instantiate the 'DecisionTreeClassifier'\n","# 'best_params_' returns the dictionary containing best parameter values and parameter name  \n","# 'get()' returns the value of specified parameter\n","# pass the 'random_state' to obtain the same samples for each time you run the code\n","dt_grid_model = DecisionTreeClassifier(criterion = dt_grid.best_params_.get('criterion'),\n","                                       max_depth = dt_grid.best_params_.get('max_depth'),\n","                                       max_leaf_nodes = dt_grid.best_params_.get('max_leaf_nodes'),\n","                                       min_samples_leaf = dt_grid.best_params_.get('min_samples_leaf'),\n","                                       min_samples_split = dt_grid.best_params_.get('min_samples_split'),\n","                                       random_state = 10)\n","\n","# use fit() to fit the model on the train set\n","dt_grid_model = dt_grid_model.fit(X_train, y_train)"]},{"cell_type":"markdown","metadata":{"id":"C7ly_LJ1wwxb"},"source":["#### 3. Plot the decision tree"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"6En1lXC9wwxb"},"outputs":[],"source":["# save the column names in 'labels'\n","lables = X_train.columns\n","\n","# plot the decision tree \n","fig = plt.figure(figsize=(30,30))\n","_ = tree.plot_tree(dt_grid_model, \n","                   feature_names=lables,  \n","                   class_names=[\"0\",\"1\"],\n","                   filled=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MpDGWdTawwxb"},"outputs":[],"source":["# DOT data\n","dot_data = tree.export_graphviz(dt_grid_model, out_file=None, \n","                                feature_names=lables,  \n","                                class_names=[\"0\",\"1\"],\n","                                filled=True)\n","\n","# Draw graph\n","graph = graphviz.Source(dot_data, format=\"png\") \n","graph\n"]},{"cell_type":"markdown","metadata":{"id":"p7zDaR-Nwwxc"},"source":["**4. Do predictions on the test set**"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"-Y22yzxxwwxc"},"outputs":[],"source":["# predict probabilities on the test set\n","# consider the probability of positive class by subsetting with '[:,1]'\n","y_pred_prob = dt_grid_model.predict_proba(X_test)[:,1]"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"cORd3zVlwwxc"},"outputs":[],"source":["# predict the class labels using 'X_test'\n","y_pred = dt_grid_model.predict(X_test)"]},{"cell_type":"markdown","metadata":{"id":"s7nkQzT1wwxd"},"source":["**5. Compute accuracy measures**"]},{"cell_type":"markdown","metadata":{"id":"ploNjcuTwwxd"},"source":["#### Build a confusion matrix."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2n_LA6SNwwxd"},"outputs":[],"source":["# call the function to plot the confusion matrix\n","# pass the decision tree (GridSearchCV) model to the function\n","plot_confusion_matrix(dt_grid_model)"]},{"cell_type":"markdown","metadata":{"id":"4CFJ_3fuwwxd"},"source":["**Calculate performance measures on the test set.**"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"_vxBjymnwwxd"},"outputs":[],"source":["# compute the performance measures on test data\n","# call the function 'get_test_report'\n","# pass the decision tree (GridSearchCV) model to the function\n","test_report = get_test_report(dt_grid_model)\n","\n","# print the performace measures\n","print(test_report)"]},{"cell_type":"markdown","metadata":{"id":"O0-BFzXkwwxe"},"source":["**Interpretation:** The accuracy is 72% for this model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hVbtrtxXwwxe"},"outputs":[],"source":["# compute kappa score on test set\n","# call the function 'kappa_score'\n","# pass the decision tree (GridSearchCV) model to the function\n","kappa_value = kappa_score(dt_grid_model)\n","\n","# print the kappa value\n","print(kappa_value)"]},{"cell_type":"markdown","metadata":{"id":"A2GfXgTTwwxe"},"source":["**Interpretation:** As the kappa score for the decision tree (GridSearchCV) is 0.4379, we can say that there is moderate agreement between the actual and predicted values."]},{"cell_type":"markdown","metadata":{"id":"DYz6XWbnwwxe"},"source":["**Plot the ROC curve.**"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"JLQhG_L4wwxe"},"outputs":[],"source":["# call the function 'plot_roc' to plot the ROC curve\n","# pass the decision tree (GridSearchCV) model to the function\n","plot_roc(dt_grid_model)"]},{"cell_type":"markdown","metadata":{"id":"3x7UFyg4wwxf"},"source":["<table align=\"left\">\n","    <tr>\n","        <td width=\"8%\">\n","            <img src=\"infer.png\">\n","        </td>\n","        <td>\n","            <div align=\"left\", style=\"font-size:120%\">\n","                <font color=\"#21618C\">\n","                    <b>The dotted line represents the ROC curve of a purely random classifier; a good classifier stays as far away from that line as possible (toward the top-left corner).<br><br>\n","We see our classifier (decision tree with GridSearchCV) to be away from the dotted line with the AUC score 0.7543.</b>\n","                </font>\n","            </div>\n","        </td>\n","    </tr>\n","</table>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"f075O_cJwwxf"},"source":["<a id=\"conclusion\"> </a>\n","# 6. Conclusion and Interpretation"]},{"cell_type":"markdown","metadata":{"id":"kCGtenJiwwxi"},"source":["To take the final conclusion, let us print the result table."]},{"cell_type":"markdown","metadata":{"id":"zTWGMSDhwwxj"},"source":["**Tabulate the results**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2udmW7hPwwxj"},"outputs":[],"source":["# use the function 'update_score_card' to store the performance measures\n","# pass the 'Decision Tree (GridSearchCV)' as model name to the function\n","update_score_card(model_name = 'Decision Tree (GridSearchCV)')"]},{"cell_type":"markdown","metadata":{"id":"S0y7axxowwxj"},"source":["<table align=\"left\">\n","    <tr>\n","        <td width=\"8%\">\n","            <img src=\"infer.png\">\n","        </td>\n","        <td>\n","            <div align=\"left\", style=\"font-size:120%\">\n","                <font color=\"#21618C\">\n","                    <b> The supervised classification learning algorithms named in the above table have been implemented on the given dataset. The performance of the models were evaluated using AUC score, accuracy, precision, f1-score, recall, and kappa score. </b>\n","                </font>\n","            </div>\n","        </td>\n","    </tr>\n","</table>\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"il5YhWRLwwxk"},"source":["<table align=\"left\">\n","    <tr>\n","        <td width=\"8%\">\n","            <img src=\"infer.png\">\n","        </td>\n","        <td>\n","            <div align=\"left\", style=\"font-size:120%\">\n","                <font color=\"#21618C\">\n","                    <b><br><br>\n","\n","The above table shows that the Decision Tree (GridSearchCV) has the highest values for most of the performance measures like AUC Score, Recall, f1-score, accuracy. Therefore, it can be concluded that the Decision Tree (GridSearchCV) can be used to predict the existence of heart disease in the patients. In future when we have more observations or bigger dataset and we can also apply ensemble techniques for prediction that can be tested again.</b>\n","                </font>\n","            </div>\n","        </td>\n","    </tr>\n","</table>"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"cwK6Hb4Qwwxk"},"outputs":[],"source":[""]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Heart Disease Prediction.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.3"}},"nbformat":4,"nbformat_minor":0}